{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing library and creating spark-session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "fileDir = \"/home/jovyan/notebooks/\"\n",
    "sys.path.append(fileDir)\n",
    "\n",
    "from utilities import *\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dependencies: \n",
      " ['neo4j-connector-apache-spark_2.12-5.0.1_for_spark_3.jar']\n"
     ]
    }
   ],
   "source": [
    "session = create_spark_session(\"Neo4j GitHub\", SparkConnector.NEO4J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS_URL = \"hdfs://namenode:9000//data-team\"\n",
    "PREFIX = \"sample_\" # \"sample_\" or \"\"\n",
    "SUFFIX = \"_1000\" # \"_10\" or \"_100\" or \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to NEO4J\n",
      "Dataframe saved to NEO4J\n",
      "Dataframe saved to NEO4J\n",
      "Dataframe saved to NEO4J\n",
      "Dataframe saved to NEO4J\n",
      "Dataframe saved to NEO4J\n",
      "Dataframe saved to NEO4J\n",
      "Dataframe saved to NEO4J\n",
      "Dataframe saved to NEO4J\n",
      "Dataframe saved to NEO4J\n",
      "Dataframe saved to NEO4J\n",
      "Dataframe saved to NEO4J\n"
     ]
    }
   ],
   "source": [
    "load_start_time = time.time()\n",
    "repositories_json = session.read.json(f\"{HDFS_URL}/{PREFIX}repositories{SUFFIX}.json\") \\\n",
    "    .withColumnRenamed(\"repo_name\", \"repo\") \\\n",
    "\n",
    "repositories_csv = session.read.csv(f\"{HDFS_URL}/repo_API_data.csv\", header=True, inferSchema=True)\n",
    "repositories_csv = repositories_csv.select(\"repo_name\",\"stargazers_count\",\"topics\")\n",
    "\n",
    "repositories = repositories_json.join(repositories_csv, repositories_json.repo == repositories_csv.repo_name, \"left\") \\\n",
    "    .select(repositories_json[\"repo\"].alias(\"repo_name\"), \n",
    "            repositories_json[\"watch_count\"], repositories_csv[\"stargazers_count\"], \n",
    "            repositories_csv[\"topics\"])\n",
    "\n",
    "languages = session.read.json(f\"{HDFS_URL}/{PREFIX}languages{SUFFIX}.json\")\n",
    "\n",
    "licences = session.read.json(f\"{HDFS_URL}/{PREFIX}licences{SUFFIX}.json\")\n",
    "\n",
    "commits = session.read.json(f\"{HDFS_URL}/{PREFIX}commits{SUFFIX}.json\") # cambia se ti serve il dataset completo\n",
    "\n",
    "load_end_time = time.time()\n",
    "\n",
    "load_time = (load_end_time - load_start_time)\n",
    "\n",
    "preprc_start_time = time.time()\n",
    "git_commits = commits \\\n",
    "    .select(\"commit\", \"subject\", \"message\", \"committer.date.seconds\", \"author.date.seconds\")\n",
    "\n",
    "newColumns = [\"id\",\"title\",\"message\",\"committer_date\",\"author_date\"]\n",
    "git_commits = git_commits \\\n",
    "    .toDF(*newColumns)\n",
    "\n",
    "git_repositories = repositories \\\n",
    "    .withColumnRenamed(\"repo_name\", \"name\")\n",
    "\n",
    "git_languages = languages.withColumn(\"name\", F.explode(languages[\"language.name\"])) \\\n",
    "    .dropDuplicates([\"name\"]) \\\n",
    "    .select(\"name\")\n",
    "\n",
    "git_licenses = licences.select(\"license\") \\\n",
    "    .withColumnRenamed(\"license\",\"name\") \\\n",
    "    .dropDuplicates([\"name\"])\n",
    "\n",
    "git_contributor = commits.select(\"author.*\") \\\n",
    "    .union(commits.select(\"committer.*\")) \\\n",
    "    .dropDuplicates([\"email\"]) \\\n",
    "    .select(\"name\",\"email\")\n",
    "\n",
    "belongs_to = commits.select(\"commit\",\"repo\")\n",
    "contains = commits.select(\"repo\",\"commit\")\n",
    "\n",
    "parent = commits.select(\"commit\", \"parent\") \\\n",
    "    .withColumn(\"parent\", F.explode(commits[\"parent\"])) \\\n",
    "    .dropDuplicates([\"commit\",\"parent\"])\n",
    "\n",
    "has = licences.select(\"repo_name\", \"license\") \\\n",
    "    .dropDuplicates([\"repo_name\", \"license\"])\n",
    "\n",
    "author = commits.select(\"author.email\", \"commit\", \"author.date.seconds\") \\\n",
    "    .withColumnRenamed(\"seconds\",\"ts\")\n",
    "author = author \\\n",
    "    .filter(author[\"email\"] != \"\") \\\n",
    "    .withColumn(\"ts\", author[\"ts\"].cast(T.IntegerType())) \\\n",
    "    .dropDuplicates([\"email\",\"commit\"])\n",
    "\n",
    "committed = commits.select(\"committer.email\", \"commit\", \"committer.date.seconds\") \\\n",
    "    .withColumnRenamed(\"seconds\",\"ts\") \n",
    "committed = committed \\\n",
    "    .filter(committed[\"email\"] != \"\") \\\n",
    "    .withColumn(\"ts\", committed[\"ts\"].cast(T.IntegerType())) \\\n",
    "    .dropDuplicates([\"email\",\"commit\"])\n",
    "\n",
    "writted_in = languages.withColumn(\"lang\", F.explode(languages[\"language\"]))\n",
    "writted_in = writted_in \\\n",
    "    .withColumn(\"language\", writted_in[\"lang.name\"]) \\\n",
    "    .withColumn(\"bytes\", writted_in[\"lang.bytes\"].cast(T.IntegerType())) \\\n",
    "    .select(\"repo_name\", \"language\", \"bytes\")\n",
    "\n",
    "preproc_end_time = time.time()\n",
    "preproc_time = (preproc_end_time - preprc_start_time)\n",
    "\n",
    "writing_start_time = time.time()\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "\n",
    "options[\"node.keys\"] = \"name\"\n",
    "options[\"labels\"] = \":GitRepository\"\n",
    "options[\"schema.optimization.type\"] = \"INDEX\"\n",
    "\n",
    "spark_write(SparkConnector.NEO4J, git_repositories, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "\n",
    "options[\"node.keys\"] = \"email\"\n",
    "options[\"labels\"] = \":GitContributor\"\n",
    "options[\"schema.optimization.type\"] = \"INDEX\"\n",
    "\n",
    "spark_write(SparkConnector.NEO4J, git_contributor, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "\n",
    "options[\"node.keys\"] = \"name\"\n",
    "options[\"labels\"] = \":GitLanguage\"\n",
    "options[\"schema.optimization.type\"] = \"INDEX\"\n",
    "\n",
    "spark_write(SparkConnector.NEO4J, git_languages, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "\n",
    "options[\"node.keys\"] = \"name\"\n",
    "options[\"labels\"] = \":GitLicense\"\n",
    "options[\"schema.optimization.type\"] = \"INDEX\"\n",
    "\n",
    "spark_write(SparkConnector.NEO4J, git_licenses, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "\n",
    "options[\"node.keys\"] = \"id\"\n",
    "options[\"labels\"] = \":GitCommit\"\n",
    "options[\"schema.optimization.type\"] = \"INDEX\"\n",
    "\n",
    "spark_write(SparkConnector.NEO4J, git_commits, \"Overwrite\", options=options)\n",
    "\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "\n",
    "options[\"relationship\"] = \"BELONGS_TO\"\n",
    "options[\"relationship.save.strategy\"] = \"keys\"\n",
    "\n",
    "\n",
    "options[\"relationship.source.labels\"] = \"GitCommit\"\n",
    "options[\"relationship.source.node.keys\"] = \"commit:id\"\n",
    "options[\"relationship.source.save.mode\"] = \"Match\"\n",
    "\n",
    "options[\"relationship.target.labels\"] = \"GitRepository\"\n",
    "options[\"relationship.target.node.keys\"] = \"repo:name\"\n",
    "options[\"relationship.target.save.mode\"] = \"Match\"\n",
    "\n",
    "\n",
    "spark_write(SparkConnector.NEO4J, belongs_to, \"Append\", options=options)\n",
    "\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "\n",
    "options[\"relationship\"] = \"CONTAINS\"\n",
    "options[\"relationship.save.strategy\"] = \"keys\"\n",
    "\n",
    "options[\"relationship.source.labels\"] = \"GitrRepository\"\n",
    "options[\"relationship.source.node.keys\"] = \"repo:name\"\n",
    "options[\"relationship.source.save.mode\"] = \"Match\"\n",
    "\n",
    "options[\"relationship.target.labels\"] = \"GitCommit\"\n",
    "options[\"relationship.target.node.keys\"] = \"commit:id\"\n",
    "options[\"relationship.target.save.mode\"] = \"Match\"\n",
    "\n",
    "\n",
    "spark_write(SparkConnector.NEO4J, contains, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "\n",
    "options[\"relationship\"] = \"PARENT\"\n",
    "options[\"relationship.save.strategy\"] = \"keys\"\n",
    "\n",
    "options[\"relationship.source.labels\"] = \"GitCommit\"\n",
    "options[\"relationship.source.node.keys\"] = \"commit:id\"\n",
    "options[\"relationship.source.save.mode\"] = \"Match\"\n",
    "\n",
    "options[\"relationship.target.labels\"] = \"GitCommit\"\n",
    "options[\"relationship.target.node.keys\"] = \"parent:id\"\n",
    "options[\"relationship.target.save.mode\"] = \"Match\"\n",
    "\n",
    "spark_write(SparkConnector.NEO4J, parent, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "\n",
    "options[\"relationship\"] = \"HAS\"\n",
    "options[\"relationship.save.strategy\"] = \"keys\"\n",
    "\n",
    "options[\"relationship.source.labels\"] = \"GitRepository\"\n",
    "options[\"relationship.source.node.keys\"] = \"repo_name:name\"\n",
    "options[\"relationship.source.save.mode\"] = \"Match\"\n",
    "\n",
    "options[\"relationship.target.labels\"] = \"GitLicense\"\n",
    "options[\"relationship.target.node.keys\"] = \"license:name\"\n",
    "options[\"relationship.target.save.mode\"] = \"Match\"\n",
    "\n",
    "spark_write(SparkConnector.NEO4J, has, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "\n",
    "options[\"relationship\"] = \"AUTHOR\"\n",
    "options[\"relationship.save.strategy\"] = \"keys\"\n",
    "options[\"relationship.properties\"] = \"ts\"\n",
    "\n",
    "options[\"relationship.source.labels\"] = \"GitContributor\"\n",
    "options[\"relationship.source.node.keys\"] = \"email:email\"\n",
    "options[\"relationship.source.save.mode\"] = \"Match\"\n",
    "\n",
    "options[\"relationship.target.labels\"] = \"GitCommit\"\n",
    "options[\"relationship.target.node.keys\"] = \"commit:id\"\n",
    "options[\"relationship.target.save.mode\"] = \"Match\"\n",
    "\n",
    "spark_write(SparkConnector.NEO4J, author, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "\n",
    "options[\"relationship\"] = \"COMMITTED\"\n",
    "options[\"relationship.save.strategy\"] = \"keys\"\n",
    "options[\"relationship.properties\"] = \"ts\"\n",
    "\n",
    "options[\"relationship.source.labels\"] = \"GitContributor\"\n",
    "options[\"relationship.source.node.keys\"] = \"email:email\"\n",
    "options[\"relationship.source.save.mode\"] = \"Match\"\n",
    "\n",
    "options[\"relationship.target.labels\"] = \"GitCommit\"\n",
    "options[\"relationship.target.node.keys\"] = \"commit:id\"\n",
    "options[\"relationship.target.save.mode\"] = \"Match\"\n",
    "\n",
    "spark_write(SparkConnector.NEO4J, committed, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "\n",
    "options[\"relationship\"] = \"WRITTED_IN\"\n",
    "options[\"relationship.save.strategy\"] = \"keys\"\n",
    "options[\"relationship.properties\"] = \"bytes\"\n",
    "\n",
    "options[\"relationship.source.labels\"] = \"GitRepository\"\n",
    "options[\"relationship.source.node.keys\"] = \"repo_name:name\"\n",
    "options[\"relationship.source.save.mode\"] = \"Match\"\n",
    "\n",
    "options[\"relationship.target.labels\"] = \"GitLanguage\"\n",
    "options[\"relationship.target.node.keys\"] = \"language:name\"\n",
    "options[\"relationship.target.save.mode\"] = \"Match\"\n",
    "\n",
    "spark_write(SparkConnector.NEO4J, writted_in, \"Append\", options=options)\n",
    "writing_end_time = time.time()\n",
    "writing_time = (writing_end_time - writing_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load time: 592.001033782959 sec\n",
      "Preprocessing time: 1.1469089984893799 sec\n",
      "Writing time: 4111.822649002075 sec\n"
     ]
    }
   ],
   "source": [
    "print(f\"Load time: {load_time} sec\")\n",
    "print(f\"Preprocessing time: {preproc_time} sec\")\n",
    "print(f\"Writing time: {writing_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded from neo4j\n",
      "Scenario 1: 163.90637874603271 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(contrib=Row(<id>=26033, <labels>=['GitContributor'], name='dependabot[bot]', email='1c358da00a777d4e9898c1280ab801e2df165188@users.noreply.github.com'), repo_count=176),\n",
       " Row(contrib=Row(<id>=53118, <labels>=['GitContributor'], name='Prayag Verma', email='35a46e17bc00e93336a001ea5a30f33595fd0d03@gmail.com'), repo_count=94),\n",
       " Row(contrib=Row(<id>=47589, <labels>=['GitContributor'], name='The Gitter Badger', email='4e199b4a1c40b497a95fcd1cd896351733849949@gitter.im'), repo_count=81),\n",
       " Row(contrib=Row(<id>=136378, <labels>=['GitContributor'], name='Ikko Ashimine', email='d41f8067726d843438db002d5555099b4901d7c1@gmail.com'), repo_count=75),\n",
       " Row(contrib=Row(<id>=69981, <labels>=['GitContributor'], name='ReadmeCritic', email='76d2ec468599ff7a19f706781bc84ca0c636b360@gmail.com'), repo_count=74),\n",
       " Row(contrib=Row(<id>=85625, <labels>=['GitContributor'], name='Tim Gates', email='af3615aeb885952b6b34e8cd8afd1feb0f18c9a5@iress.com'), repo_count=49),\n",
       " Row(contrib=Row(<id>=95221, <labels>=['GitContributor'], name='Andrew Murray', email='ad908c236cd47b4e613a008504771bc8a75d3839@gmail.com'), repo_count=42),\n",
       " Row(contrib=Row(<id>=67893, <labels>=['GitContributor'], name='Morton Fox', email='64b2b6d12bfe4baae7dad3d018f8cbf6b0e7a044@qslw.com'), repo_count=42),\n",
       " Row(contrib=Row(<id>=131033, <labels>=['GitContributor'], name='Josh Soref', email='dc510c92cc1794ea84000fde88becdce67bf7624@users.noreply.github.com'), repo_count=40),\n",
       " Row(contrib=Row(<id>=77625, <labels>=['GitContributor'], name='greenkeeperio-bot', email='5bdcd3c0d4d24ae3e71b3b452a024c6324c7e4bb@greenkeeper.io'), repo_count=36)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scenario 1\n",
    "start_time = time.time()\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "options[\"query\"] =  \"\"\"\n",
    "                        MATCH (contrib:GitContributor)-[:AUTHOR]->(commit:GitCommit)-[:BELONGS_TO]->(repo:GitRepository)\n",
    "                        WITH contrib, COUNT(DISTINCT repo) as repo_count\n",
    "                        RETURN contrib, repo_count ORDER BY repo_count DESC\n",
    "                    \"\"\"\n",
    "top10contributors = spark_read(SparkConnector.NEO4J, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 1: {end_time - start_time} sec\")\n",
    "display(top10contributors.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded from neo4j\n",
      "Scenario 2: 0.6484222412109375 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(repo_name='tensorflow/tensorflow', lang='C++', percOfBytes=0.63),\n",
       " Row(repo_name='godotengine/godot', lang='C++', percOfBytes=0.88),\n",
       " Row(repo_name='grpc/grpc', lang='C++', percOfBytes=0.65),\n",
       " Row(repo_name='google/ion', lang='C++', percOfBytes=0.9),\n",
       " Row(repo_name='google/googletest', lang='C++', percOfBytes=0.9),\n",
       " Row(repo_name='alibaba/AndFix', lang='C++', percOfBytes=0.62),\n",
       " Row(repo_name='facebook/redex', lang='C++', percOfBytes=0.87),\n",
       " Row(repo_name='facebook/folly', lang='C++', percOfBytes=0.95),\n",
       " Row(repo_name='johang/btfs', lang='C++', percOfBytes=0.85),\n",
       " Row(repo_name='tesseract-ocr/tesseract', lang='C++', percOfBytes=0.96)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scenario 2:\n",
    "\n",
    "start_time = time.time()\n",
    "LANGUAGE = \"C++\"\n",
    "PERCENTAGE = 0.5\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "options[\"query\"] = f\"\"\"\n",
    "                    MATCH (r:GitRepository)-[w:WRITTED_IN]->(l:GitLanguage)\n",
    "                    WITH r, SUM(w.bytes) AS totalBytesForRepo, collect({{language_name:l.name,bytes: w.bytes}}) AS bytesForLanguages\n",
    "                    UNWIND bytesForLanguages AS bytesForLanguage\n",
    "                    WITH r.name AS repo_name, bytesForLanguage.language_name AS lang, round((bytesForLanguage.bytes*1.0/totalBytesForRepo),2) AS percOfBytes\n",
    "                        WHERE lang = \"{LANGUAGE}\" AND percOfBytes > {PERCENTAGE}\n",
    "                    RETURN repo_name, lang, percOfBytes \n",
    "                  \"\"\"\n",
    "bytesPercentageInRepos = spark_read(SparkConnector.NEO4J, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 2: {end_time - start_time} sec\")\n",
    "display(bytesPercentageInRepos.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded from neo4j\n",
      "Scenario 3: 0.8158688545227051 sec\n",
      "+----------+\n",
      "|mergeCount|\n",
      "+----------+\n",
      "|     12127|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scenario 3:\n",
    "start_time = time.time()\n",
    "REPO_NAME = \"tensorflow/tensorflow\"\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "options[\"query\"] =  f\"\"\"\n",
    "                        MATCH (repository:GitRepository {{name: \"{REPO_NAME}\"}})<-[:BELONGS_TO]-(commit:GitCommit), \n",
    "                            r = (commit)-[:PARENT]->()\n",
    "                        WITH commit, collect(r) AS parents\n",
    "                        WHERE size(parents) > 1\n",
    "                        RETURN count(commit) AS mergeCount\n",
    "                    \"\"\"\n",
    "bytesPercentageInRepos = spark_read(SparkConnector.NEO4J, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 3: {end_time - start_time} sec\")\n",
    "bytesPercentageInRepos.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Put query text in the neo4j GUI",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_140/1163168496.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0mPROJ_NAME\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"contribRepoAndCommits\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Put query text in the neo4j GUI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: Put query text in the neo4j GUI"
     ]
    }
   ],
   "source": [
    "# Scenario 4 - project graph contributors, commits and repos:\n",
    "query = \"\"\"     \n",
    "        CALL gds.graph.project.cypher(\n",
    "              'contribRepoAndCommits',\n",
    "              'MATCH (n) WHERE n:GitContributor OR n:GitCommit OR n:GitRepository RETURN ID(n) AS id',\n",
    "              'MATCH (n)-[r]->(m) WHERE r:PARENT OR r:BELONGS_TO OR r:COMMITTED OR r:AUTHOR RETURN ID(n) AS source, ID(m) AS target'\n",
    "        )\n",
    "        \"\"\"\n",
    "PROJ_NAME=\"contribRepoAndCommits\"\n",
    "raise Exception(\"Put query text in the neo4j GUI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded from neo4j\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "options[\"query\"] =  f\"\"\"\n",
    "                        MATCH (n)-[r]->(m) WHERE r:PARENT OR r:BELONGS_TO OR r:COMMITTED OR r:AUTHOR RETURN ID(n) AS source, ID(m) AS target \n",
    "                    \"\"\"\n",
    "louvain = spark_read(SparkConnector.NEO4J, session, options=options)\n",
    "louvain.write.option(\"header\", True).mode(\"overwrite\").csv(\"hdfs://namenode:9000//data-team/edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded from neo4j\n",
      "Scenario 4: 123.08715415000916 sec\n"
     ]
    }
   ],
   "source": [
    "# Scenario 4:\n",
    "start_time = time.time()\n",
    "PROJ_NAME = \"contribRepoAndCommits\"\n",
    "\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "options[\"query\"] =  f\"\"\"\n",
    "                        CALL gds.labelPropagation.stream('{PROJ_NAME}')\n",
    "                        YIELD nodeId, communityId\n",
    "                        RETURN nodeId as ID, communityId \n",
    "                    \"\"\"\n",
    "louvain = spark_read(SparkConnector.NEO4J, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 4: {end_time - start_time} sec\")\n",
    "louvain.write.option(\"header\", True).mode(\"overwrite\").csv(\"hdfs://namenode:9000//data-team/louvain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded from neo4j\n",
      "Scenario 5: 350.89533853530884 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "options = get_default_options(SparkConnector.NEO4J)\n",
    "options[\"query\"] = f\"\"\"\n",
    "                        CALL gds.pageRank.stream('contribRepoAndCommits')\n",
    "                        YIELD nodeId, score\n",
    "                        RETURN gds.util.asNode(nodeId).name as name, score\n",
    "                        ORDER BY score DESC\n",
    "                    \"\"\"\n",
    "louvain = spark_read(SparkConnector.NEO4J, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 5: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='tensorflow/tensorflow', score=15630.063491839835),\n",
       " Row(name='apple/swift', score=15427.057959425001),\n",
       " Row(name='kubernetes/kubernetes', score=12170.155320558157),\n",
       " Row(name='dotnet/roslyn', score=10044.6008681273),\n",
       " Row(name='Microsoft/vscode', score=9750.781070613648),\n",
       " Row(name='rails/rails', score=9693.129889366524),\n",
       " Row(name='Homebrew/homebrew', score=7593.8009527367085),\n",
       " Row(name='symfony/symfony', score=6869.918907495444),\n",
       " Row(name='ansible/ansible', score=6393.512080307458),\n",
       " Row(name='golang/go', score=6262.886339651076)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(louvain.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sparkContext.stop()\n",
    "session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
