{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing library and creating spark-session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyGithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "fileDir = \"/home/jovyan/notebooks/\"\n",
    "sys.path.append(fileDir)\n",
    "\n",
    "from utilities import *\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = create_spark_session(\"Tigergraph GitHub\", SparkConnector.TIGERGRAPH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS_URL = \"hdfs://namenode:9000//data-team\"\n",
    "PREFIX = \"sample_\" # \"sample_\" or \"\"\n",
    "SUFFIX = \"_100\" # \"_10\" or \"_100\" or \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_start_time = time.time()\n",
    "repositories_json = session.read.json(f\"{HDFS_URL}/{PREFIX}repositories{SUFFIX}.json\") \\\n",
    "    .withColumnRenamed(\"repo_name\", \"repo\") \\\n",
    "\n",
    "repositories_csv = session.read.csv(f\"{HDFS_URL}/repo_API_data.csv\", header=True, inferSchema=True)\n",
    "repositories_csv = repositories_csv.select(\"repo_name\",\"forks_count\",\"open_issues_count\",\"stargazers_count\",\"topics\")\n",
    "\n",
    "repositories = repositories_json.join(repositories_csv, repositories_json.repo == repositories_csv.repo_name, \"left\") \\\n",
    "    .select(repositories_json[\"repo\"].alias(\"repo_name\"), \n",
    "            repositories_json[\"watch_count\"], repositories_csv[\"stargazers_count\"], \n",
    "            repositories_csv[\"topics\"], repositories_csv[\"forks_count\"], repositories_csv[\"open_issues_count\"])\n",
    "# set 0 as efault value for stargazers_count and forks_count\n",
    "repositories = repositories.na.fill(0, [\"stargazers_count\", \"watch_count\", \"topics\", \"forks_count\", \"open_issues_count\"])\n",
    "\n",
    "languages = session.read.json(f\"{HDFS_URL}/{PREFIX}languages{SUFFIX}.json\")\n",
    "\n",
    "licences = session.read.json(f\"{HDFS_URL}/{PREFIX}licences{SUFFIX}.json\")\n",
    "\n",
    "commits = session.read.json(f\"{HDFS_URL}/{PREFIX}commits{SUFFIX}.json\") # cambia se ti serve il dataset completo\n",
    "\n",
    "load_end_time = time.time()\n",
    "load_time = (load_end_time - load_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprc_start_time = time.time()\n",
    "\n",
    "git_commits = commits.select( \n",
    "    commits[\"commit\"].alias(\"v_id\"),\n",
    "commits[\"subject\"].alias(\"title\"), \n",
    "\"message\")\n",
    "\n",
    "git_repositories = repositories.withColumnRenamed(\"repo_name\", \"v_id\")\n",
    "git_repositories = git_repositories.withColumn(\"watch_count\", git_repositories[\"watch_count\"].cast(T.IntegerType())) \\\n",
    "    .withColumn(\"forks_count\", git_repositories[\"forks_count\"].cast(T.IntegerType())) \\\n",
    "    .withColumn(\"stargazers_count\", git_repositories[\"stargazers_count\"].cast(T.IntegerType())) \\\n",
    "    .withColumn(\"open_issues_count\", git_repositories[\"open_issues_count\"].cast(T.IntegerType())) \n",
    "\n",
    "git_languages = languages.withColumn(\"v_id\", F.explode(languages[\"language.name\"])).dropDuplicates([\"v_id\"]).select(\"v_id\")\n",
    "\n",
    "git_licenses = licences.select(\"license\").withColumnRenamed(\"license\",\"v_id\").dropDuplicates([\"v_id\"])\n",
    "\n",
    "git_contributor = commits.select(\"author.*\") \\\n",
    "    .union(commits.select(\"committer.*\")) \\\n",
    "    .dropDuplicates([\"email\"])\n",
    "git_contributor = git_contributor.select(\"email\",\"name\") \\\n",
    "    .filter(git_contributor[\"email\"]!=\"\") \\\n",
    "    .withColumnRenamed(\"email\",\"v_id\")\n",
    "\n",
    "belongs_to = commits.select(\"commit\",\"repo\") \\\n",
    "    .withColumnRenamed(\"commit\",\"GitCommit\") \\\n",
    "    .withColumnRenamed(\"repo\",\"GitRepository\")\n",
    "\n",
    "contains = belongs_to.select(\"GitRepository\",\"GitCommit\")\n",
    "\n",
    "parent = commits.select(\"commit\", \"parent\") \\\n",
    "    .withColumn(\"parent\", F.explode(commits[\"parent\"])) \\\n",
    "    .withColumnRenamed(\"commit\",\"GitCommit\") \\\n",
    "    .withColumnRenamed(\"parent\",\"GitCommit\")\\\n",
    "    .dropDuplicates([\"GitCommit\",\"GitCommit\"])\n",
    "\n",
    "has = licences.select(\"repo_name\", \"license\") \\\n",
    "    .withColumnRenamed(\"repo_name\",\"GitRepository\") \\\n",
    "    .withColumnRenamed(\"license\",\"GitLicense\")\\\n",
    "    .dropDuplicates([\"GitRepository\",\"GitLicense\"])\n",
    "\n",
    "author = commits.select(\"author.email\", \"commit\", \"author.date.seconds\") \\\n",
    "    .withColumnRenamed(\"email\",\"GitContributor\") \\\n",
    "    .withColumnRenamed(\"commit\",\"GitCommit\") \\\n",
    "    .withColumnRenamed(\"seconds\",\"ts\") \n",
    "author = author \\\n",
    "    .filter(author[\"GitContributor\"]!=\"\") \\\n",
    "    .withColumn(\"ts\", author[\"ts\"].cast(T.IntegerType())) \\\n",
    "    .dropDuplicates([\"GitContributor\",\"GitCommit\"])\n",
    "\n",
    "committed = commits.select(\"committer.email\", \"commit\", \"committer.date.seconds\") \\\n",
    "    .withColumnRenamed(\"email\",\"GitContributor\") \\\n",
    "    .withColumnRenamed(\"commit\",\"GitCommit\") \\\n",
    "    .withColumnRenamed(\"seconds\",\"ts\") \n",
    "committed = committed \\\n",
    "    .filter(committed[\"GitContributor\"]!=\"\") \\\n",
    "    .withColumn(\"ts\", committed[\"ts\"].cast(T.IntegerType())) \\\n",
    "    .dropDuplicates([\"GitContributor\",\"GitCommit\"])\n",
    "\n",
    "writted_in = languages.withColumn(\"language\", F.explode(languages[\"language\"]))\n",
    "writted_in = writted_in \\\n",
    "    .withColumn(\"GitLanguage\", writted_in[\"language.name\"]) \\\n",
    "    .withColumn(\"bytes\", writted_in[\"language.bytes\"].cast(T.IntegerType())) \\\n",
    "    .withColumnRenamed(\"repo_name\", \"GitRepository\") \\\n",
    "    .select(\"GitRepository\", \"GitLanguage\", \"bytes\")\n",
    "\n",
    "\n",
    "preproc_end_time = time.time()\n",
    "preproc_time = (preproc_end_time - preprc_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writing_start_time = time.time()\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"vertex GitRepository\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, git_repositories, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"vertex GitContributor\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, git_contributor, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"vertex GitLanguage\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, git_languages, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"vertex GitLicense\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, git_licenses, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"vertex GitCommit\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, git_commits, \"Append\", options=options)\n",
    "## Writing the relationships in the graph\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge BELONGS_TO\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, belongs_to, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge CONTAINS\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, contains, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge HAS\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, has, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge PARENT\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, parent, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge WRITTEN_IN\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, writted_in, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge AUTHOR\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, author, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge COMMITTED\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, committed, \"Append\", options=options)\n",
    "\n",
    "writing_end_time = time.time()\n",
    "writing_time = (writing_end_time - writing_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Load time: {load_time} sec\")\n",
    "print(f\"Preprocessing time: {preproc_time} sec\")\n",
    "print(f\"Writing time: {writing_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1\n",
    "N = 10\n",
    "start_time = time.time()\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "options[\"dbtable\"] = f'query TopNAuthorsWithMoreContributes(N={N})'\n",
    "top10contributors = spark_read(SparkConnector.TIGERGRAPH, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 1: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2:\n",
    "LANGUAGE = \"C++\"\n",
    "PERCENTAGE = 0.5\n",
    "\n",
    "start_time = time.time()\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "options[\"dbtable\"] = f'query ReposWithMoreThenPercentageOnLenguage(perc={PERCENTAGE}, lang=\"{LANGUAGE}\")'\n",
    "repos = spark_read(SparkConnector.TIGERGRAPH, session, options=options)\n",
    "bytesPercentageInRepos = spark_read(SparkConnector.TIGERGRAPH, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 2: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3:\n",
    "REPO_NAME = \"tensorflow/tensorflow\"\n",
    "start_time = time.time()\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "options[\"dbtable\"] = f'query CountMergeCommits(repo_name=\"{REPO_NAME}\")'\n",
    "repos = spark_read(SparkConnector.TIGERGRAPH, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 3: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 4 - Louvain:\n",
    "start_time = time.time()\n",
    "v_type_set = '[\"GitContributor\", \"GitRepository\", \"GitCommit\"]'\n",
    "e_type_set = '[\"CONTAINS\", \"PARENT\", \"BELONGS_TO\"]'\n",
    "\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "options[\"dbtable\"] = f'query tg_label_prop( \\\n",
    "    v_type_set={v_type_set}, \\\n",
    "    e_type_set={e_type_set}, \\\n",
    "    maximum_iteration=250, \\\n",
    "    print_limit=-1, \\\n",
    "    print_results=TRUE, file_path=\"\", result_attribute=\"\")'\n",
    "communities = spark_read(SparkConnector.TIGERGRAPH, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 4 louvain: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 5: page rank contributori\n",
    "start_time = time.time()\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "options[\"dbtable\"] = f'query tg_pagerank(\"GitCommit\", \"PARENT\", 0.001, 25, 0.85, 10, _, _, _, _)'\n",
    "top10contributors = spark_read(SparkConnector.TIGERGRAPH, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 5: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sparkContext.stop()\n",
    "session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
