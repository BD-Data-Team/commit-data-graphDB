{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing library and creating spark-session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyGithub\n",
      "  Downloading PyGithub-1.59.1-py3-none-any.whl (342 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.2/342.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pynacl>=1.4.0\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.14.0 in /opt/conda/lib/python3.7/site-packages (from PyGithub) (2.28.1)\n",
      "Collecting deprecated\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: pyjwt[crypto]>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from PyGithub) (2.5.0)\n",
      "Requirement already satisfied: cryptography>=3.3.1 in /opt/conda/lib/python3.7/site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (38.0.1)\n",
      "Collecting types-cryptography>=3.3.21\n",
      "  Downloading types_cryptography-3.3.23.2-py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: cffi>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from pynacl>=1.4.0->PyGithub) (1.15.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.14.0->PyGithub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.14.0->PyGithub) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.14.0->PyGithub) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.14.0->PyGithub) (1.26.11)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n",
      "Installing collected packages: types-cryptography, wrapt, pynacl, deprecated, PyGithub\n",
      "Successfully installed PyGithub-1.59.1 deprecated-1.2.14 pynacl-1.5.0 types-cryptography-3.3.23.2 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install PyGithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "fileDir = \"/home/jovyan/notebooks/\"\n",
    "sys.path.append(fileDir)\n",
    "\n",
    "from utilities import *\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dependencies: \n",
      " ['postgresql-42.5.0.jar', 'tigergraph-jdbc-driver-1.3.6.jar']\n"
     ]
    }
   ],
   "source": [
    "session = create_spark_session(\"Tigergraph GitHub\", SparkConnector.TIGERGRAPH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS_URL = \"hdfs://namenode:9000//data-team\"\n",
    "PREFIX = \"sample_\" # \"sample_\" or \"\"\n",
    "SUFFIX = \"_100\" # \"_10\" or \"_100\" or \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to TIGERGRAPH\n",
      "Dataframe saved to TIGERGRAPH\n",
      "Dataframe saved to TIGERGRAPH\n",
      "Dataframe saved to TIGERGRAPH\n",
      "Dataframe saved to TIGERGRAPH\n",
      "Dataframe saved to TIGERGRAPH\n",
      "Dataframe saved to TIGERGRAPH\n",
      "Dataframe saved to TIGERGRAPH\n",
      "Dataframe saved to TIGERGRAPH\n",
      "Dataframe saved to TIGERGRAPH\n",
      "Dataframe saved to TIGERGRAPH\n",
      "Dataframe saved to TIGERGRAPH\n"
     ]
    }
   ],
   "source": [
    "load_start_time = time.time()\n",
    "repositories_json = session.read.json(f\"{HDFS_URL}/{PREFIX}repositories{SUFFIX}.json\") \\\n",
    "    .withColumnRenamed(\"repo_name\", \"repo\") \\\n",
    "\n",
    "repositories_csv = session.read.csv(f\"{HDFS_URL}/repo_API_data.csv\", header=True, inferSchema=True)\n",
    "repositories_csv = repositories_csv.select(\"repo_name\",\"forks_count\",\"open_issues_count\",\"stargazers_count\",\"topics\")\n",
    "\n",
    "repositories = repositories_json.join(repositories_csv, repositories_json.repo == repositories_csv.repo_name, \"left\") \\\n",
    "    .select(repositories_json[\"repo\"].alias(\"repo_name\"), \n",
    "            repositories_json[\"watch_count\"], repositories_csv[\"stargazers_count\"], \n",
    "            repositories_csv[\"topics\"], repositories_csv[\"forks_count\"], repositories_csv[\"open_issues_count\"])\n",
    "# set 0 as efault value for stargazers_count and forks_count\n",
    "repositories = repositories.na.fill(0, [\"stargazers_count\", \"watch_count\", \"topics\", \"forks_count\", \"open_issues_count\"])\n",
    "\n",
    "languages = session.read.json(f\"{HDFS_URL}/{PREFIX}languages{SUFFIX}.json\")\n",
    "\n",
    "licences = session.read.json(f\"{HDFS_URL}/{PREFIX}licences{SUFFIX}.json\")\n",
    "\n",
    "commits = session.read.json(f\"{HDFS_URL}/{PREFIX}commits{SUFFIX}.json\") # cambia se ti serve il dataset completo\n",
    "\n",
    "load_end_time = time.time()\n",
    "load_time = (load_end_time - load_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprc_start_time = time.time()\n",
    "\n",
    "git_commits = commits.select( \n",
    "    commits[\"commit\"].alias(\"v_id\"),\n",
    "commits[\"subject\"].alias(\"title\"), \n",
    "\"message\")\n",
    "\n",
    "git_repositories = repositories.withColumnRenamed(\"repo_name\", \"v_id\")\n",
    "git_repositories = git_repositories.withColumn(\"watch_count\", git_repositories[\"watch_count\"].cast(T.IntegerType())) \\\n",
    "    .withColumn(\"forks_count\", git_repositories[\"forks_count\"].cast(T.IntegerType())) \\\n",
    "    .withColumn(\"stargazers_count\", git_repositories[\"stargazers_count\"].cast(T.IntegerType())) \\\n",
    "    .withColumn(\"open_issues_count\", git_repositories[\"open_issues_count\"].cast(T.IntegerType())) \n",
    "\n",
    "git_languages = languages.withColumn(\"v_id\", F.explode(languages[\"language.name\"])).dropDuplicates([\"v_id\"]).select(\"v_id\")\n",
    "\n",
    "git_licenses = licences.select(\"license\").withColumnRenamed(\"license\",\"v_id\").dropDuplicates([\"v_id\"])\n",
    "\n",
    "git_contributor = commits.select(\"author.*\") \\\n",
    "    .union(commits.select(\"committer.*\")) \\\n",
    "    .dropDuplicates([\"email\"])\n",
    "git_contributor = git_contributor.select(\"email\",\"name\") \\\n",
    "    .filter(git_contributor[\"email\"]!=\"\") \\\n",
    "    .withColumnRenamed(\"email\",\"v_id\")\n",
    "\n",
    "belongs_to = commits.select(\"commit\",\"repo\") \\\n",
    "    .withColumnRenamed(\"commit\",\"GitCommit\") \\\n",
    "    .withColumnRenamed(\"repo\",\"GitRepository\")\n",
    "\n",
    "contains = belongs_to.select(\"GitRepository\",\"GitCommit\")\n",
    "\n",
    "parent = commits.select(\"commit\", \"parent\") \\\n",
    "    .withColumn(\"parent\", F.explode(commits[\"parent\"])) \\\n",
    "    .withColumnRenamed(\"commit\",\"GitCommit\") \\\n",
    "    .withColumnRenamed(\"parent\",\"GitCommit\")\\\n",
    "    .dropDuplicates([\"GitCommit\",\"GitCommit\"])\n",
    "\n",
    "has = licences.select(\"repo_name\", \"license\") \\\n",
    "    .withColumnRenamed(\"repo_name\",\"GitRepository\") \\\n",
    "    .withColumnRenamed(\"license\",\"GitLicense\")\\\n",
    "    .dropDuplicates([\"GitRepository\",\"GitLicense\"])\n",
    "\n",
    "author = commits.select(\"author.email\", \"commit\", \"author.date.seconds\") \\\n",
    "    .withColumnRenamed(\"email\",\"GitContributor\") \\\n",
    "    .withColumnRenamed(\"commit\",\"GitCommit\") \\\n",
    "    .withColumnRenamed(\"seconds\",\"ts\") \n",
    "author = author \\\n",
    "    .filter(author[\"GitContributor\"]!=\"\") \\\n",
    "    .withColumn(\"ts\", author[\"ts\"].cast(T.IntegerType())) \\\n",
    "    .dropDuplicates([\"GitContributor\",\"GitCommit\"])\n",
    "\n",
    "committed = commits.select(\"committer.email\", \"commit\", \"committer.date.seconds\") \\\n",
    "    .withColumnRenamed(\"email\",\"GitContributor\") \\\n",
    "    .withColumnRenamed(\"commit\",\"GitCommit\") \\\n",
    "    .withColumnRenamed(\"seconds\",\"ts\") \n",
    "committed = committed \\\n",
    "    .filter(committed[\"GitContributor\"]!=\"\") \\\n",
    "    .withColumn(\"ts\", committed[\"ts\"].cast(T.IntegerType())) \\\n",
    "    .dropDuplicates([\"GitContributor\",\"GitCommit\"])\n",
    "\n",
    "writted_in = languages.withColumn(\"language\", F.explode(languages[\"language\"]))\n",
    "writted_in = writted_in \\\n",
    "    .withColumn(\"GitLanguage\", writted_in[\"language.name\"]) \\\n",
    "    .withColumn(\"bytes\", writted_in[\"language.bytes\"].cast(T.IntegerType())) \\\n",
    "    .withColumnRenamed(\"repo_name\", \"GitRepository\") \\\n",
    "    .select(\"GitRepository\", \"GitLanguage\", \"bytes\")\n",
    "\n",
    "\n",
    "preproc_end_time = time.time()\n",
    "preproc_time = (preproc_end_time - preprc_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writing_start_time = time.time()\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"vertex GitRepository\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, git_repositories, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"vertex GitContributor\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, git_contributor, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"vertex GitLanguage\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, git_languages, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"vertex GitLicense\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, git_licenses, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"vertex GitCommit\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, git_commits, \"Append\", options=options)\n",
    "## Writing the relationships in the graph\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge BELONGS_TO\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, belongs_to, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge CONTAINS\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, contains, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge HAS\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, has, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge PARENT\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, parent, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge WRITTEN_IN\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, writted_in, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge AUTHOR\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, author, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "\n",
    "options[\"dbtable\"] = \"edge COMMITTED\"\n",
    "\n",
    "spark_write(SparkConnector.TIGERGRAPH, committed, \"Append\", options=options)\n",
    "\n",
    "writing_end_time = time.time()\n",
    "writing_time = (writing_end_time - writing_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load time: 21.67391324043274 sec\n",
      "Preprocessing time: 0.8274252414703369 sec\n",
      "Writing time: 1609.4341628551483 sec\n"
     ]
    }
   ],
   "source": [
    "print(f\"Load time: {load_time} sec\")\n",
    "print(f\"Preprocessing time: {preproc_time} sec\")\n",
    "print(f\"Writing time: {writing_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1\n",
    "N = 10\n",
    "start_time = time.time()\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "options[\"dbtable\"] = f'query TopNAuthorsWithMoreContributes(N={N})'\n",
    "top10contributors = spark_read(SparkConnector.TIGERGRAPH, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 1: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2:\n",
    "LANGUAGE = \"C++\"\n",
    "PERCENTAGE = 0.5\n",
    "\n",
    "start_time = time.time()\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "options[\"dbtable\"] = f'query ReposWithMoreThenPercentageOnLenguage(perc={PERCENTAGE}, lang=\"{LANGUAGE}\")'\n",
    "repos = spark_read(SparkConnector.TIGERGRAPH, session, options=options)\n",
    "bytesPercentageInRepos = spark_read(SparkConnector.TIGERGRAPH, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 2: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3:\n",
    "REPO_NAME = \"tensorflow/tensorflow\"\n",
    "start_time = time.time()\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "options[\"dbtable\"] = f'query CountMergeCommits(repo_name=\"{REPO_NAME}\")'\n",
    "repos = spark_read(SparkConnector.TIGERGRAPH, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 3: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_119/3880268391.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     print_results=TRUE, file_path=\"\", result_attribute=\"\")'\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcommunities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSparkConnector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTIGERGRAPH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Scenario 4 louvain: {end_time - start_time} sec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/utilities.py\u001b[0m in \u001b[0;36mspark_read\u001b[0;34m(connector, session, options)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     def json(\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Scenario 4 - Louvain:\n",
    "start_time = time.time()\n",
    "v_type_set = '[\"GitContributor\", \"GitRepository\", \"GitCommit\"]'\n",
    "e_type_set = '[\"CONTAINS\", \"PARENT\", \"BELONGS_TO\"]'\n",
    "\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "options[\"dbtable\"] = f'query tg_label_prop( \\\n",
    "    v_type_set={v_type_set}, \\\n",
    "    e_type_set={e_type_set}, \\\n",
    "    maximum_iteration=250, \\\n",
    "    print_limit=-1, \\\n",
    "    print_results=TRUE, file_path=\"\", result_attribute=\"\")'\n",
    "communities = spark_read(SparkConnector.TIGERGRAPH, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 4 louvain: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_119/1344265389.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSparkConnector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTIGERGRAPH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dbtable\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'query tg_pagerank(\"GitCommit\", \"PARENT\", 0.001, 25, 0.85, 10, _, _, _, _)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtop10contributors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSparkConnector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTIGERGRAPH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Scenario 5: {end_time - start_time} sec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/utilities.py\u001b[0m in \u001b[0;36mspark_read\u001b[0;34m(connector, session, options)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     def json(\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Scenario 5: page rank contributori\n",
    "start_time = time.time()\n",
    "options = get_default_options(SparkConnector.TIGERGRAPH)\n",
    "options[\"dbtable\"] = f'query tg_pagerank(\"GitCommit\", \"PARENT\", 0.001, 25, 0.85, 10, _, _, _, _)'\n",
    "top10contributors = spark_read(SparkConnector.TIGERGRAPH, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 5: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sparkContext.stop()\n",
    "session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
