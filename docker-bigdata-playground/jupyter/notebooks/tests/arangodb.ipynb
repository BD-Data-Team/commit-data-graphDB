{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "fileDir = \"/home/jovyan/notebooks/\"\n",
    "sys.path.append(fileDir)\n",
    "\n",
    "from utilities import *\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dependencies: \n",
      " ['arangodb-java-driver-shaded-7.1.0.jar', 'arangodb-spark-commons-3.3_2.12-1.5.0.jar', 'arangodb-spark-datasource-3.3_2.12-1.5.0.jar', 'commons-codec-1.11.jar', 'commons-logging-1.2.jar', 'httpclient-4.5.13.jar', 'httpcore-4.4.13.jar', 'jackson-dataformat-velocypack-4.1.0.jar', 'slf4j-api-2.0.7.jar']\n"
     ]
    }
   ],
   "source": [
    "session = create_spark_session(\"ARANGODB GitHub\", SparkConnector.ARANGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS_URL = \"hdfs://namenode:9000//data-team\"\n",
    "PREFIX = \"sample_\" # \"sample_\" or \"\"\n",
    "SUFFIX = \"_100\" # \"_10\" or \"_100\" or \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_start_time = time.time()\n",
    "repositories_json = session.read.json(f\"{HDFS_URL}/{PREFIX}repositories{SUFFIX}.json\", encoding='unicode_escape') \\\n",
    "    .withColumnRenamed(\"repo_name\", \"repo\") \\\n",
    "\n",
    "repositories_csv = session.read.csv(f\"{HDFS_URL}/repo_API_data.csv\", header=True, inferSchema=True, encoding='unicode_escape')\n",
    "repositories_csv = repositories_csv.select(\"repo_name\",\"stargazers_count\",\"topics\")\n",
    "\n",
    "repositories = repositories_json.join(repositories_csv, repositories_json.repo == repositories_csv.repo_name, \"left\") \\\n",
    "    .select(repositories_json[\"repo\"].alias(\"repo_name\"), \n",
    "            repositories_json[\"watch_count\"], repositories_csv[\"stargazers_count\"], \n",
    "            repositories_csv[\"topics\"])\n",
    "\n",
    "languages = session.read.json(f\"{HDFS_URL}/{PREFIX}languages{SUFFIX}.json\", encoding='unicode_escape')\n",
    "\n",
    "licences = session.read.json(f\"{HDFS_URL}/{PREFIX}licences{SUFFIX}.json\", encoding='unicode_escape')\n",
    "\n",
    "commits = session.read.json(f\"{HDFS_URL}/{PREFIX}commits{SUFFIX}.json\", encoding='unicode_escape')\n",
    "\n",
    "load_end_time = time.time()\n",
    "\n",
    "load_time = (load_end_time - load_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/dataframe.py:3315: FutureWarning: DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\n",
      "  FutureWarning,\n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "/usr/local/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/spark/python/pyspark/sql/dataframe.py:3315: FutureWarning: DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_5118/919889244.py\", line 13, in remove_at_and_blank\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x94 in position 50: invalid start byte\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5118/919889244.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mauthor_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medges_pd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mauthor_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_df_columns_nullable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"_from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_to\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0medges_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"committer_email\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"email\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"commit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"committer.date.seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/utilities.py\u001b[0m in \u001b[0;36mset_df_columns_nullable\u001b[0;34m(spark, df, column_list, nullable)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstruct_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mstruct_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnullable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnullable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mdf_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;34m\"\"\"Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_rdd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjavaToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             self._lazy_rdd = RDD(\n\u001b[1;32m    177\u001b[0m                 \u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_5118/919889244.py\", line 13, in remove_at_and_blank\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x94 in position 50: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preprc_start_time = time.time()\n",
    "\n",
    "def remove_back(text):\n",
    "    return text.replace(\"/\", \"::\").replace(r\"[^a-zA-Z0-9:]\", \"\")\n",
    "\n",
    "remove_udf = F.udf(remove_back, T.StringType())\n",
    "repositories = repositories.withColumn(\"repo_name\", remove_udf(\"repo_name\"))\n",
    "commits = commits.withColumn(\"repo\", remove_udf(\"repo\"))\n",
    "licences = licences.withColumn(\"repo_name\", remove_udf(\"repo_name\"))\n",
    "languages = languages.withColumn(\"repo_name\", remove_udf(\"repo_name\"))\n",
    "def remove_at_and_blank(text):\n",
    "    return text.replace(\"@\", \"::\").replace(r\"[^a-zA-Z0-9:]\", \"\")\n",
    "\n",
    "remove_at_and_blank = F.udf(remove_at_and_blank, T.StringType())\n",
    "commits = commits.withColumn(\"author_email\", remove_at_and_blank(\"author.email\"))\n",
    "commits = commits.withColumn(\"committer_email\", remove_at_and_blank(\"committer.email\"))\n",
    "\n",
    "### Data Processing\n",
    "git_commits= commits.select(\"commit\", \"subject\", \"message\")\n",
    "newColumns = [\"_key\", \"title\", \"message\"]\n",
    "git_commits = git_commits.toDF(*newColumns)\n",
    "\n",
    "git_repositories = repositories.withColumnRenamed(\"repo_name\", \"_key\")\n",
    "\n",
    "def remove_c_sharp(text):\n",
    "    return text.replace(\"#\", \"s\").replace(\" \", \"\").replace(\"++\", \"pp\")\n",
    "\n",
    "remove_c_sharp = F.udf(remove_c_sharp, T.StringType())\n",
    "\n",
    "git_languages = languages.withColumn(\"name\", F.explode(languages[\"language.name\"]))\\\n",
    "    .dropDuplicates([\"name\"])\\\n",
    "    .select(\"name\")\\\n",
    "    .withColumnRenamed(\"name\", \"_key\")\n",
    "\n",
    "git_languages = git_languages.withColumn(\"_key\", remove_c_sharp(git_languages[\"_key\"]))\n",
    "\n",
    "git_licenses = licences.select(\"license\").withColumnRenamed(\n",
    "    \"license\", \"name\").dropDuplicates([\"name\"]).withColumnRenamed(\"name\", \"_key\")\n",
    "\n",
    "\n",
    "git_contributor = commits.select(\"author.name\",commits[\"author_email\"].alias(\"email\")) \\\n",
    "    .union(commits.select(\"committer.name\", commits[\"committer_email\"].alias(\"email\"))) \\\n",
    "    .dropDuplicates([\"name\"]) \\\n",
    "    .select(\"name\", \"email\")\\\n",
    "    .withColumnRenamed(\"email\", \"_key\")\n",
    "\n",
    "git_contributor = git_contributor.filter(git_contributor[\"_key\"] != \"\")\n",
    "\n",
    "edges_df = commits.select(\"commit\", \"repo\")\\\n",
    "                .withColumnRenamed(\"commit\", \"_from\")\\\n",
    "                .withColumnRenamed(\"repo\", \"_to\")\\\n",
    "                .withColumn(\"_to\", remove_udf(\"_to\"))\n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitCommit/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitRepository/\" + edges_pd_df[\"_to\"]\n",
    "belongs_to_df = edges_pd_df.to_spark()\n",
    "belongs_to_df = set_df_columns_nullable(session, belongs_to_df, [\"_from\", \"_to\"], False)\n",
    "\n",
    "edges_df = commits.select(\"commit\", \"parent\") \\\n",
    "    .withColumn(\"parent\", F.explode(commits[\"parent\"])) \\\n",
    "    .withColumnRenamed(\"commit\", \"_from\")\\\n",
    "    .withColumnRenamed(\"parent\", \"_to\")\\\n",
    "    .dropDuplicates([\"_from\", \"_to\"])\n",
    "\n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitCommit/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitCommit/\" + edges_pd_df[\"_to\"]\n",
    "\n",
    "parent_df = edges_pd_df.to_spark()\n",
    "parent_df = set_df_columns_nullable(session, parent_df, [\"_from\", \"_to\"], False)\n",
    "\n",
    "edges_df = licences.select(\"repo_name\", \"license\") \\\n",
    "    .dropDuplicates([\"repo_name\", \"license\"]) \\\n",
    "    .withColumnRenamed(\"repo_name\", \"_from\")\\\n",
    "    .withColumnRenamed(\"license\", \"_to\")\\\n",
    "    .dropDuplicates([\"_from\", \"_to\"])\n",
    "    \n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitRepository/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitLicense/\" + edges_pd_df[\"_to\"]\n",
    "\n",
    "\n",
    "has_df = edges_pd_df.to_spark()\n",
    "has_df = set_df_columns_nullable(session, has_df, [\"_from\", \"_to\"], False)\n",
    "\n",
    "edges_df = commits.select(commits[\"author_email\"].alias(\"email\"), \"commit\", \"author.date.seconds\") \\\n",
    "    .withColumnRenamed(\"seconds\", \"ts\")\n",
    "edges_df = edges_df \\\n",
    "    .filter(edges_df[\"email\"] != \"\") \\\n",
    "    .withColumn(\"ts\", edges_df[\"ts\"].cast(T.IntegerType())) \\\n",
    "    .withColumnRenamed(\"email\", \"_from\")\\\n",
    "    .withColumnRenamed(\"commit\", \"_to\")\\\n",
    "    .dropDuplicates([\"_from\", \"_to\"])\n",
    "\n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitContributor/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitCommit/\" + edges_pd_df[\"_to\"]\n",
    "\n",
    "\n",
    "author_df = edges_pd_df.to_spark()\n",
    "author_df = set_df_columns_nullable(session, author_df, [\"_from\", \"_to\"], False)\n",
    "\n",
    "edges_df = commits.select(commits[\"committer_email\"].alias(\"email\"), \"commit\", \"committer.date.seconds\") \\\n",
    "    .withColumnRenamed(\"seconds\", \"ts\")\n",
    "edges_df = edges_df \\\n",
    "    .filter(edges_df[\"email\"] != \"\") \\\n",
    "    .withColumn(\"ts\", edges_df[\"ts\"].cast(T.IntegerType())) \\\n",
    "    .withColumnRenamed(\"email\", \"_from\")\\\n",
    "    .withColumnRenamed(\"commit\", \"_to\")\\\n",
    "    .dropDuplicates([\"_from\", \"_to\"])\n",
    "\n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitContributor/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitCommit/\" + edges_pd_df[\"_to\"]\n",
    "\n",
    "\n",
    "committed_df = edges_pd_df.to_spark()\n",
    "committed_df = set_df_columns_nullable(session, committed_df, [\"_from\", \"_to\"], False)\n",
    "\n",
    "edges_df = languages.withColumn(\"lang\", F.explode(languages[\"language\"]))\n",
    "edges_df = edges_df \\\n",
    "    .withColumn(\"language\", edges_df[\"lang.name\"]) \\\n",
    "    .withColumn(\"bytes\", edges_df[\"lang.bytes\"].cast(T.IntegerType())) \\\n",
    "    .select(\"repo_name\", \"language\", \"bytes\") \\\n",
    "    .withColumnRenamed(\"repo_name\", \"_from\")\\\n",
    "    .withColumnRenamed(\"language\", \"_to\")\\\n",
    "    .dropDuplicates([\"_from\", \"_to\"])\n",
    "\n",
    "edges_df = edges_df.withColumn(\"_to\", remove_c_sharp(edges_df[\"_to\"]))\n",
    "\n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitRepository/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitLanguage/\" + edges_pd_df[\"_to\"]\n",
    "\n",
    "writted_in_df = edges_pd_df.to_spark()\n",
    "writted_in_df = set_df_columns_nullable(session, writted_in_df, [\"_from\", \"_to\"], False)\n",
    "\n",
    "\n",
    "preproc_end_time = time.time()\n",
    "preproc_time = (preproc_end_time - preprc_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writing_start_time = time.time()\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"document\"\n",
    "options[\"table\"] = \"GitCommit\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, git_commits, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"document\"\n",
    "options[\"table\"] = \"GitRepository\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, git_repositories, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"document\"\n",
    "options[\"table\"] = \"GitContributor\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, git_contributor, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"document\"\n",
    "options[\"table\"] = \"GitLanguage\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, git_languages, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"document\"\n",
    "options[\"table\"] = \"GitLicense\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, git_licenses, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"BELONGS_TO\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, belongs_to_df, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"PARENT\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, parent_df, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"HAS\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, has_df, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"AUTHOR\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, author_df, \"Append\", options=options)\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"COMMITTED\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, committed_df, \"Overwrite\", options=options)\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"WRITTEN_IN\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, writted_in_df, \"Overwrite\", options=options)\n",
    "\n",
    "writing_end_time = time.time()\n",
    "writing_time = (writing_end_time - writing_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Load time: {load_time} sec\")\n",
    "print(f\"Preprocessing time: {preproc_time} sec\")\n",
    "print(f\"Writing time: {writing_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1\n",
    "N = 10\n",
    "start_time = time.time()\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"query\"] = f\"LET distinctValues = (\\\n",
    "                        FOR c IN GitContributor\\\n",
    "                            FOR commit IN OUTBOUND c AUTHOR\\\n",
    "                                FOR r IN OUTBOUND commit BELONGS_TO\\\n",
    "                                    RETURN DISTINCT{{c, r}})\\\n",
    "                    FOR d in distinctValues\\\n",
    "                        COLLECT contrib = d.c.name WITH COUNT INTO repo_count\\\n",
    "                        SORT repo_count DESC\\\n",
    "                        FILTER repo_count > 1\\\n",
    "                        LIMIT {N}\\\n",
    "                        RETURN {{contrib, repo_count}}\"\n",
    "\n",
    "df = spark_read(SparkConnector.ARANGO, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 1: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2:\n",
    "\n",
    "LANGUAGE = \"C++\"\n",
    "BYTES_PERCENTAGE = 0.5\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query = f\"\"\"\n",
    "    FOR repo IN GitRepository\n",
    "    LET repoTotalBytes = (\n",
    "    FOR lan IN OUTBOUND repo WRITTEN_IN\n",
    "        LET byteInfo = (\n",
    "                FOR info IN WRITTEN_IN\n",
    "                FILTER info._from == repo._id AND info._to == lan._id\n",
    "                RETURN info.bytes\n",
    "            )\n",
    "        COLLECT repository = repo._key\n",
    "        AGGREGATE repoTotalBytes = SUM(byteInfo[0])\n",
    "        RETURN {{repository, repoTotalBytes}}\n",
    "        )\n",
    "    FILTER LENGTH(repoTotalBytes) > 0 //for the repos with 0 WRITTEN_IN edges. FIX mini-batch\n",
    "    \n",
    "    FOR lan IN OUTBOUND repo WRITTEN_IN\n",
    "        LET byteInfo = (\n",
    "          FOR info IN WRITTEN_IN\n",
    "            FILTER info._from == repo._id AND info._to == lan._id\n",
    "            RETURN info.bytes\n",
    "        )\n",
    "        COLLECT repo_name = repo._key, language = lan._key, percentageOfBytes = (byteInfo[0]/repoTotalBytes[0].repoTotalBytes)\n",
    "        FILTER language == \"{LANGUAGE}\" AND percentageOfBytes > {BYTES_PERCENTAGE}\n",
    "        RETURN {{\n",
    "          repo_name, \n",
    "          language,\n",
    "          percentageOfBytes\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"query\"] = query\n",
    "\n",
    "df = spark_read(SparkConnector.ARANGO, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 2: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3:\n",
    "REPO_NAME = \"tensorflow::tensorflow\"\n",
    "\n",
    "start_time = time.time()\n",
    "query = f\"\"\"\n",
    "        FOR repo IN GitRepository\n",
    "            FILTER repo._key == \"{REPO_NAME}\"\n",
    "            FOR commit IN INBOUND repo BELONGS_TO\n",
    "                LET parents = ( \n",
    "                FOR parent IN OUTBOUND commit PARENT\n",
    "                    COLLECT comm = commit._key INTO parents\n",
    "                    RETURN {{lun: length(parents), comm}}\n",
    "                )\n",
    "            FILTER parents[0].lun>1 AND parents[0].comm == commit._key\n",
    "            COLLECT WITH COUNT INTO n_merge\n",
    "            RETURN {{num_merge: n_merge}}\n",
    "        \"\"\"\n",
    "\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"query\"] = query\n",
    "\n",
    "df = spark_read(SparkConnector.ARANGO, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 3: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arango import ArangoClient\n",
    "\n",
    "# Initialize the ArangoDB client.\n",
    "client = ArangoClient(hosts='http://arangodb:8529')\n",
    "\n",
    "# Connect to \"test\" database as root user.\n",
    "db = client.db('_system', username='root', password='')\n",
    "\n",
    "# Get the Pregel API wrapper.\n",
    "pregel = db.pregel\n",
    "\n",
    "\n",
    "# Scenario 4:\n",
    "start_time = time.time()\n",
    "\n",
    "# Start a new Pregel job.\n",
    "job_id = db.pregel.create_job(\n",
    "    graph='github_graph',\n",
    "    algorithm='labelpropagation',\n",
    "    store=False,\n",
    "    max_gss=250,\n",
    "    thread_count=1,\n",
    "    async_mode=False,\n",
    "    result_field='community'\n",
    ")\n",
    "\n",
    "# Retrieve details of a Pregel job by ID.\n",
    "job = pregel.job(job_id)\n",
    "query = f\"\"\"\n",
    "        FOR v IN PREGEL_RESULT({job[\"id\"]})\n",
    "        RETURN {{key: v._key,\n",
    "                community: v.community}}\n",
    "        \"\"\"\n",
    "\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"query\"] = query\n",
    "\n",
    "df = spark_read(SparkConnector.ARANGO, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 4: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Scenario 5: page rank contributori\n",
    "start_time = time.time()\n",
    "query = f\"\"\"\n",
    "        FOR v IN PREGEL_RESULT({job[\"id\"]})\n",
    "        RETURN {{key: v._key,\n",
    "                community: v.community}}\n",
    "        \"\"\"\n",
    "\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"query\"] = query\n",
    "\n",
    "df = spark_read(SparkConnector.ARANGO, session, options=options)\n",
    "end_time = time.time()\n",
    "print(f\"Scenario 5: {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sparkContext.stop()\n",
    "session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
