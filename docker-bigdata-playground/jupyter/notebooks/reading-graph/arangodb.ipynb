{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "fileDir = \"/home/jovyan/notebooks/\"\n",
    "sys.path.append(fileDir)\n",
    "\n",
    "from utilities import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import pandas as ps\n",
    "import pyspark.sql.types as T\n",
    "from arango import ArangoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 4 - creating job\n",
    "\n",
    "# Initialize the ArangoDB client.\n",
    "client = ArangoClient(hosts='http://arangodb:8529')\n",
    "\n",
    "# Connect to \"test\" database as root user.\n",
    "db = client.db('_system', username='root', password='')\n",
    "\n",
    "# Get the Pregel API wrapper.\n",
    "pregel = db.pregel\n",
    "\n",
    "# Start a new Pregel job in \"school\" graph.\n",
    "job_id = db.pregel.create_job(\n",
    "    graph='github_graph',\n",
    "    algorithm='labelpropagation',\n",
    "    store=False,\n",
    "    max_gss=250,\n",
    "    thread_count=1,\n",
    "    async_mode=False,\n",
    "    result_field='community'\n",
    ")\n",
    "\n",
    "# Retrieve details of a Pregel job by ID.\n",
    "job = pregel.job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dependencies: \n",
      " ['arangodb-java-driver-shaded-7.1.0.jar', 'arangodb-spark-commons-3.3_2.12-1.5.0.jar', 'arangodb-spark-datasource-3.3_2.12-1.5.0.jar', 'commons-codec-1.11.jar', 'commons-logging-1.2.jar', 'httpclient-4.5.13.jar', 'httpcore-4.4.13.jar', 'jackson-dataformat-velocypack-4.1.0.jar', 'slf4j-api-2.0.7.jar']\n"
     ]
    }
   ],
   "source": [
    "session = create_spark_session(\"ArangoDB GitHub\", SparkConnector.ARANGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded from arangodb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(contrib='Ikko Ashimine', repo_count=4),\n",
       " Row(contrib='dependabot[bot]', repo_count=3),\n",
       " Row(contrib='MichaÃ«l De Boey', repo_count=3),\n",
       " Row(contrib='Kohei TAKATA', repo_count=3),\n",
       " Row(contrib='Ronald Eddy Jr', repo_count=3),\n",
       " Row(contrib='C. T. Lin', repo_count=3),\n",
       " Row(contrib='James George', repo_count=3),\n",
       " Row(contrib='Prayag Verma', repo_count=3),\n",
       " Row(contrib='James Reggio', repo_count=2),\n",
       " Row(contrib='Shingo Sato', repo_count=2)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scenario 1\n",
    "\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"query\"] = \"LET distinctValues = (\\\n",
    "                        FOR c IN GitContributor\\\n",
    "                            FOR commit IN OUTBOUND c AUTHOR\\\n",
    "                                FOR r IN OUTBOUND commit BELONGS_TO\\\n",
    "                                    RETURN DISTINCT{c, r})\\\n",
    "                    FOR d in distinctValues\\\n",
    "                        COLLECT contrib = d.c.name WITH COUNT INTO repo_count\\\n",
    "                        SORT repo_count DESC\\\n",
    "                        FILTER repo_count > 1\\\n",
    "                        LIMIT 10\\\n",
    "                        RETURN {contrib, repo_count}\"\n",
    "\n",
    "df = spark_read(SparkConnector.ARANGO, session, options=options)\n",
    "display(df.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find contributors who have contributed to the largest number of repositories\n",
    "\n",
    "#options = get_default_options(SparkConnector.ARANGO)\n",
    "#options[\"query\"] = \"FOR contributor IN GitContributor\\\n",
    "#                        FILTER contributor._key == 'f08905f3496a7b7d60e0da97307f6ad7594abc92::users.noreply.github.com'\\\n",
    "#                        FOR commit IN OUTBOUND contributor COMMITTED\\\n",
    "#                            RETURN {contributor: contributor, commit: commit}\"\n",
    "\n",
    "#df = spark_read(SparkConnector.ARANGO, session, options=options)\n",
    "#display(df.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded from arangodb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(language='JavaScript', percentageOfBytes=0.9577282863121208, repo_name='facebook::react'),\n",
       " Row(language='JavaScript', percentageOfBytes=0.6090344172523278, repo_name='FreeCodeCamp::FreeCodeCamp')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scenario 2\n",
    "LANGUAGE = \"JavaScript\"\n",
    "BYTES_PERCENTAGE = 0.5\n",
    "\n",
    "query = f\"\"\"\n",
    "    FOR repo IN GitRepository\n",
    "    LET repoTotalBytes = (\n",
    "    FOR lan IN OUTBOUND repo WRITTEN_IN\n",
    "        LET byteInfo = (\n",
    "                FOR info IN WRITTEN_IN\n",
    "                FILTER info._from == repo._id AND info._to == lan._id\n",
    "                RETURN info.bytes\n",
    "            )\n",
    "        COLLECT repository = repo._key\n",
    "        AGGREGATE repoTotalBytes = SUM(byteInfo[0])\n",
    "        RETURN {{repository, repoTotalBytes}}\n",
    "        )\n",
    "    FILTER LENGTH(repoTotalBytes) > 0 //for the repos with 0 WRITTEN_IN edges. FIX mini-batch\n",
    "    \n",
    "    FOR lan IN OUTBOUND repo WRITTEN_IN\n",
    "        LET byteInfo = (\n",
    "          FOR info IN WRITTEN_IN\n",
    "            FILTER info._from == repo._id AND info._to == lan._id\n",
    "            RETURN info.bytes\n",
    "        )\n",
    "        COLLECT repo_name = repo._key, language = lan._key, percentageOfBytes = (byteInfo[0]/repoTotalBytes[0].repoTotalBytes)\n",
    "        FILTER language == \"{LANGUAGE}\" AND percentageOfBytes > {BYTES_PERCENTAGE}\n",
    "        RETURN {{\n",
    "          repo_name, \n",
    "          language,\n",
    "          percentageOfBytes\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"query\"] = query\n",
    "\n",
    "df = spark_read(SparkConnector.ARANGO, session, options=options)\n",
    "display(df.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded from arangodb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(num_merge=12127)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scenario 3\n",
    "REPO_NAME = \"tensorflow::tensorflow\"\n",
    "\n",
    "query = f\"\"\"\n",
    "        FOR repo IN GitRepository\n",
    "            FILTER repo._key == \"{REPO_NAME}\"\n",
    "            FOR commit IN INBOUND repo BELONGS_TO\n",
    "                LET parents = ( \n",
    "                FOR parent IN OUTBOUND commit PARENT\n",
    "                    COLLECT comm = commit._key INTO parents\n",
    "                    RETURN {{lun: length(parents), comm}}\n",
    "                )\n",
    "            FILTER parents[0].lun>1 AND parents[0].comm == commit._key\n",
    "            COLLECT WITH COUNT INTO n_merge\n",
    "            RETURN {{num_merge: n_merge}}\n",
    "        \"\"\"\n",
    "\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"query\"] = query\n",
    "\n",
    "df = spark_read(SparkConnector.ARANGO, session, options=options)\n",
    "display(df.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6747565'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job[\"id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded from arangodb\n",
      "+---------+--------------------+\n",
      "|community|                 key|\n",
      "+---------+--------------------+\n",
      "|        0|32ccc6a0699d6cdb4...|\n",
      "|   214052|ec7ef50e8b7a61639...|\n",
      "|     5287|e14bd919f4190cda8...|\n",
      "|        3|7841569d4d48db86b...|\n",
      "|        4|71f558ffc155d70e0...|\n",
      "|        5|4d5677a66efdc13a4...|\n",
      "|     6754|5b70f3abdf2b74808...|\n",
      "|     1837|15502617a56103750...|\n",
      "|     4657|5ab86342e63388663...|\n",
      "|     7332|e06e0764e33dd3840...|\n",
      "|      829|f4e223bcebdbda8a4...|\n",
      "|       11|e193db130bc4d662c...|\n",
      "|      610|d5fd0b2931c85ac39...|\n",
      "|       13|1047980dca0830cd5...|\n",
      "|       14|b11dd87047f40c4c4...|\n",
      "|      717|f27624a1e9e89dca3...|\n",
      "|       16|a4ca98ed322d4290f...|\n",
      "|     4698|be8b0acdd0cee84c5...|\n",
      "|       18|0e42ad5756e4675b5...|\n",
      "|       19|e90710ad4640668fa...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scenario 4 - getting job result\n",
    "query = f\"\"\"\n",
    "        FOR v IN PREGEL_RESULT({job[\"id\"]})\n",
    "        RETURN {{key: v._key,\n",
    "                community: v.community}}\n",
    "        \"\"\"\n",
    "\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"query\"] = query\n",
    "\n",
    "df = spark_read(SparkConnector.ARANGO, session, options=options)\n",
    "display(df.show(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 5:\n",
    "job_id = db.pregel.create_job(\n",
    "    graph='github_graph',\n",
    "    algorithm='pagerank',\n",
    "    store=False,\n",
    "    max_gss=250,\n",
    "    thread_count=1,\n",
    "    async_mode=False,\n",
    "    result_field='result',\n",
    "    algorithm_params={'threshold': 0.000001}\n",
    ")\n",
    "\n",
    "# Retrieve details of a Pregel job by ID.\n",
    "job = pregel.job(job_id)\n",
    "\n",
    "query = f\"\"\"\n",
    "        FOR v IN PREGEL_RESULT({job[\"id\"]})\n",
    "        RETURN {{key: v._key,\n",
    "                rank: v.result}}\n",
    "        \"\"\"\n",
    "\n",
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"query\"] = query\n",
    "\n",
    "df = spark_read(SparkConnector.ARANGO, session, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sparkContext.stop()\n",
    "session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
