{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "fileDir = \"/home/jovyan/notebooks/\"\n",
    "sys.path.append(fileDir)\n",
    "\n",
    "from utilities import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import pandas as ps\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dependencies: \n",
      " ['arangodb-java-driver-shaded-7.1.0.jar', 'arangodb-spark-commons-3.3_2.12-1.5.0.jar', 'arangodb-spark-datasource-3.3_2.12-1.5.0.jar', 'commons-codec-1.11.jar', 'commons-logging-1.2.jar', 'httpclient-4.5.13.jar', 'httpcore-4.4.13.jar', 'jackson-dataformat-velocypack-4.1.0.jar', 'slf4j-api-2.0.7.jar']\n"
     ]
    }
   ],
   "source": [
    "session = create_spark_session(\"ArangoDB GitHub\", SparkConnector.ARANGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_URI = \"hdfs://namenode:9000//data-team\"\n",
    "prefix = \"sample_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from the HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- bytes: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- repo_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# togli prefix se ti serve il dataset completo\n",
    "languages = session.read.json(f\"{common_URI}/{prefix}languages.json\")\n",
    "languages.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- repo_name: string (nullable = true)\n",
      " |-- watch_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# togli prefix se ti serve il dataset completo\n",
    "repositories = session.read.json(f\"{common_URI}/{prefix}repositories.json\")\n",
    "repositories.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- license: string (nullable = true)\n",
      " |-- repo_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "licences = session.read.json(f\"{common_URI}/{prefix}licences.json\")\n",
    "licences.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- ref: string (nullable = true)\n",
      " |-- repo_name: string (nullable = true)\n",
      " |-- symlink_target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = session.read.json(f\"{common_URI}/{prefix}files.json\")\n",
    "files.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: struct (nullable = true)\n",
      " |    |-- date: struct (nullable = true)\n",
      " |    |    |-- seconds: string (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- time_sec: string (nullable = true)\n",
      " |    |-- tz_offset: string (nullable = true)\n",
      " |-- commit: string (nullable = true)\n",
      " |-- committer: struct (nullable = true)\n",
      " |    |-- date: struct (nullable = true)\n",
      " |    |    |-- seconds: string (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- time_sec: string (nullable = true)\n",
      " |    |-- tz_offset: string (nullable = true)\n",
      " |-- difference: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- new_mode: string (nullable = true)\n",
      " |    |    |-- new_path: string (nullable = true)\n",
      " |    |    |-- new_repo: string (nullable = true)\n",
      " |    |    |-- new_sha1: string (nullable = true)\n",
      " |    |    |-- old_mode: string (nullable = true)\n",
      " |    |    |-- old_path: string (nullable = true)\n",
      " |    |    |-- old_sha1: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- parent: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- repo: string (nullable = true)\n",
      " |-- repo_name: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- trailer: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- value: string (nullable = true)\n",
      " |-- tree: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commits = session.read.json(f\"{common_URI}/{prefix}commits.json\")\n",
    "commits.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_back(text):\n",
    "    return text.replace(\"/\", \"::\")\n",
    "\n",
    "remove_udf = F.udf(remove_back, T.StringType())\n",
    "repositories = repositories.withColumn(\"repo_name\", remove_udf(\"repo_name\"))\n",
    "files = files.withColumn(\"repo_name\", remove_udf(\"repo_name\"))\n",
    "commits = commits.withColumn(\"repo\", remove_udf(\"repo\"))\n",
    "licences = licences.withColumn(\"repo_name\", remove_udf(\"repo_name\"))\n",
    "languages = languages.withColumn(\"repo_name\", remove_udf(\"repo_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_at_and_blank(text):\n",
    "    return text.replace(\"@\", \"::\").replace(\" \", \"\")\n",
    "\n",
    "remove_at_and_blank = F.udf(remove_at_and_blank, T.StringType())\n",
    "commits = commits.withColumn(\"author_email\", remove_at_and_blank(\"author.email\"))\n",
    "commits = commits.withColumn(\"committer_email\", remove_at_and_blank(\"committer.email\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _key: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "git_commits= commits.select(\"commit\", \"subject\", \"message\")\n",
    "newColumns = [\"_key\", \"title\", \"message\"]\n",
    "git_commits = git_commits.toDF(*newColumns)\n",
    "git_commits.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                _key|watch_count|\n",
      "+--------------------+-----------+\n",
      "|FreeCodeCamp::Fre...|      90457|\n",
      "|    firehol::netdata|      13208|\n",
      "|    joshbuchea::HEAD|      13125|\n",
      "|braydie::HowToBeA...|      12019|\n",
      "|sindresorhus::awe...|      11063|\n",
      "|tensorflow::tenso...|      10728|\n",
      "|     facebook::react|      10458|\n",
      "|ParsePlatform::pa...|      10339|\n",
      "|  loverajoel::jstips|       9585|\n",
      "|facebook::react-n...|       9437|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "git_repositories = repositories.withColumnRenamed(\"repo_name\", \"_key\")\n",
    "git_repositories.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _key: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_c_sharp(text):\n",
    "    return text.replace(\"#\", \"s\").replace(\" \", \"\").replace(\"++\", \"pp\")\n",
    "\n",
    "remove_c_sharp = F.udf(remove_c_sharp, T.StringType())\n",
    "\n",
    "git_languages = languages.withColumn(\"name\", F.explode(languages[\"language.name\"]))\\\n",
    "    .dropDuplicates([\"name\"])\\\n",
    "    .select(\"name\")\\\n",
    "    .withColumnRenamed(\"name\", \"_key\")\n",
    "\n",
    "git_languages = git_languages.withColumn(\"_key\", remove_c_sharp(git_languages[\"_key\"]))\n",
    "git_languages.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _key: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "git_licenses = licences.select(\"license\").withColumnRenamed(\n",
    "    \"license\", \"name\").dropDuplicates([\"name\"]).withColumnRenamed(\"name\", \"_key\")\n",
    "git_licenses.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _key: string (nullable = true)\n",
      " |-- ref: string (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- symlink_target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "git_files = files.select(\"id\", \"ref\", \"path\", \"mode\", \"symlink_target\").withColumnRenamed(\"id\", \"_key\")\n",
    "git_files.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- _key: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "git_contributor = commits.select(\"author.name\",commits[\"author_email\"].alias(\"email\")) \\\n",
    "    .union(commits.select(\"committer.name\", commits[\"committer_email\"].alias(\"email\"))) \\\n",
    "    .dropDuplicates([\"name\"]) \\\n",
    "    .select(\"name\", \"email\")\\\n",
    "    .withColumnRenamed(\"email\", \"_key\")\n",
    "\n",
    "git_contributor = git_contributor.filter(git_contributor[\"_key\"] != \"\")\n",
    "git_contributor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the nodes in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"document\"\n",
    "options[\"table\"] = \"GitCommit\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, git_commits, \"Overwrite\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"document\"\n",
    "options[\"table\"] = \"GitRepository\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, git_repositories, \"Overwrite\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"document\"\n",
    "options[\"table\"] = \"GitContributor\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, git_contributor, \"Overwrite\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"document\"\n",
    "options[\"table\"] = \"GitLanguage\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, git_languages, \"Overwrite\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"document\"\n",
    "options[\"table\"] = \"GitLicense\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, git_licenses, \"Overwrite\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"document\"\n",
    "options[\"table\"] = \"GitFile\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, git_files, \"Overwrite\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the relationships in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/dataframe.py:3315: FutureWarning: DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _from: string (nullable = false)\n",
      " |-- _to: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "edges_df = commits.select(\"commit\", \"repo\")\\\n",
    "                .withColumnRenamed(\"commit\", \"_from\")\\\n",
    "                .withColumnRenamed(\"repo\", \"_to\")\\\n",
    "                .withColumn(\"_to\", remove_udf(\"_to\"))\n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitCommit/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitRepository/\" + edges_pd_df[\"_to\"]\n",
    "belongs_to_df = edges_pd_df.to_spark()\n",
    "belongs_to_df = set_df_columns_nullable(session, belongs_to_df, [\"_from\", \"_to\"], False)\n",
    "belongs_to_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _from: string (nullable = false)\n",
      " |-- _to: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df = files.select(\"id\", \"repo_name\")\\\n",
    "    .withColumnRenamed(\"id\", \"_from\")\\\n",
    "    .withColumnRenamed(\"repo_name\", \"_to\")\\\n",
    "    .dropDuplicates([\"_from\", \"_to\"])\n",
    "\n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitFile/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitRepository/\" + edges_pd_df[\"_to\"]\n",
    "\n",
    "stays_in_df = edges_pd_df.to_spark()\n",
    "stays_in_df = set_df_columns_nullable(session, stays_in_df, [\"_from\", \"_to\"], False)\n",
    "stays_in_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _from: string (nullable = false)\n",
      " |-- _to: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df = commits.select(\"commit\", \"parent\") \\\n",
    "    .withColumn(\"parent\", F.explode(commits[\"parent\"])) \\\n",
    "    .withColumnRenamed(\"commit\", \"_from\")\\\n",
    "    .withColumnRenamed(\"parent\", \"_to\")\\\n",
    "    .dropDuplicates([\"_from\", \"_to\"])\n",
    "\n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitCommit/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitCommit/\" + edges_pd_df[\"_to\"]\n",
    "\n",
    "parent_df = edges_pd_df.to_spark()\n",
    "parent_df = set_df_columns_nullable(session, parent_df, [\"_from\", \"_to\"], False)\n",
    "parent_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _from: string (nullable = false)\n",
      " |-- _to: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df = licences.select(\"repo_name\", \"license\") \\\n",
    "    .dropDuplicates([\"repo_name\", \"license\"]) \\\n",
    "    .withColumnRenamed(\"repo_name\", \"_from\")\\\n",
    "    .withColumnRenamed(\"license\", \"_to\")\\\n",
    "    .dropDuplicates([\"_from\", \"_to\"])\n",
    "    \n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitRepository/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitLicense/\" + edges_pd_df[\"_to\"]\n",
    "\n",
    "\n",
    "has_df = edges_pd_df.to_spark()\n",
    "has_df = set_df_columns_nullable(session, has_df, [\"_from\", \"_to\"], False)\n",
    "has_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _from: string (nullable = false)\n",
      " |-- _to: string (nullable = false)\n",
      " |-- ts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df = commits.select(commits[\"author_email\"].alias(\"email\"), \"commit\", \"author.date.seconds\") \\\n",
    "    .withColumnRenamed(\"seconds\", \"ts\")\n",
    "edges_df = edges_df \\\n",
    "    .filter(edges_df[\"email\"] != \"\") \\\n",
    "    .withColumn(\"ts\", edges_df[\"ts\"].cast(T.IntegerType())) \\\n",
    "    .withColumnRenamed(\"email\", \"_from\")\\\n",
    "    .withColumnRenamed(\"commit\", \"_to\")\\\n",
    "    .dropDuplicates([\"_from\", \"_to\"])\n",
    "\n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitContributor/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitCommit/\" + edges_pd_df[\"_to\"]\n",
    "\n",
    "\n",
    "author_df = edges_pd_df.to_spark()\n",
    "author_df = set_df_columns_nullable(session, author_df, [\"_from\", \"_to\"], False)\n",
    "author_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _from: string (nullable = false)\n",
      " |-- _to: string (nullable = false)\n",
      " |-- ts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df = commits.select(commits[\"committer_email\"].alias(\"email\"), \"commit\", \"committer.date.seconds\") \\\n",
    "    .withColumnRenamed(\"seconds\", \"ts\")\n",
    "edges_df = edges_df \\\n",
    "    .filter(edges_df[\"email\"] != \"\") \\\n",
    "    .withColumn(\"ts\", edges_df[\"ts\"].cast(T.IntegerType())) \\\n",
    "    .withColumnRenamed(\"email\", \"_from\")\\\n",
    "    .withColumnRenamed(\"commit\", \"_to\")\\\n",
    "    .dropDuplicates([\"_from\", \"_to\"])\n",
    "\n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitContributor/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitCommit/\" + edges_pd_df[\"_to\"]\n",
    "\n",
    "\n",
    "committed_df = edges_pd_df.to_spark()\n",
    "committed_df = set_df_columns_nullable(session, committed_df, [\"_from\", \"_to\"], False)\n",
    "committed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _from: string (nullable = false)\n",
      " |-- _to: string (nullable = false)\n",
      " |-- bytes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df = languages.withColumn(\"lang\", F.explode(languages[\"language\"]))\n",
    "edges_df = edges_df \\\n",
    "    .withColumn(\"language\", edges_df[\"lang.name\"]) \\\n",
    "    .withColumn(\"bytes\", edges_df[\"lang.bytes\"].cast(T.IntegerType())) \\\n",
    "    .select(\"repo_name\", \"language\", \"bytes\") \\\n",
    "    .withColumnRenamed(\"repo_name\", \"_from\")\\\n",
    "    .withColumnRenamed(\"language\", \"_to\")\\\n",
    "    .dropDuplicates([\"_from\", \"_to\"])\n",
    "\n",
    "edges_df = edges_df.withColumn(\"_to\", remove_c_sharp(edges_df[\"_to\"]))\n",
    "\n",
    "edges_pd_df = edges_df.to_pandas_on_spark()\n",
    "edges_pd_df[\"_from\"] = \"GitRepository/\" + edges_pd_df[\"_from\"]\n",
    "edges_pd_df[\"_to\"] = \"GitLanguage/\" + edges_pd_df[\"_to\"]\n",
    "\n",
    "\n",
    "writted_in_df = edges_pd_df.to_spark()\n",
    "writted_in_df = set_df_columns_nullable(session, writted_in_df, [\"_from\", \"_to\"], False)\n",
    "writted_in_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"BELONGS_TO\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, belongs_to_df, \"Overwrite\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"STAYS_IN\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, stays_in_df, \"Overwrite\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"PARENT\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, parent_df, \"Overwrite\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"HAS\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, has_df, \"Overwrite\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"AUTHOR\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, author_df, \"Append\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"COMMITTED\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, committed_df, \"Overwrite\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ARANGO\n"
     ]
    }
   ],
   "source": [
    "options = get_default_options(SparkConnector.ARANGO)\n",
    "options[\"table.type\"] = \"edge\"\n",
    "options[\"table\"] = \"WRITTEN_IN\"\n",
    "options[\"createCollection\"] = \"true\"\n",
    "\n",
    "spark_write(SparkConnector.ARANGO, writted_in_df, \"Overwrite\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop spark context and spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sparkContext.stop()\n",
    "session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
